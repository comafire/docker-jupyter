{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia\n",
    "## Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 0.6.2\n",
      "Commit d386e40c17 (2017-12-13 18:08 UTC)\n",
      "Platform Info:\n",
      "  OS: Linux (x86_64-pc-linux-gnu)\n",
      "  CPU: Intel(R) Core(TM) i7-5820K CPU @ 3.30GHz\n",
      "  WORD_SIZE: 64\n",
      "  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell)\n",
      "  LAPACK: libopenblas64_\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-3.9.1 (ORCJIT, haswell)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPrecompiling module DataStructures.\n",
      "\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /usr/local/jdk1.8.0_172/jre/lib/amd64/server/libjvm.so\n",
      "2018-04-23 16:45:00 INFO  SparkContext:54 - Running Spark version 2.3.0\n",
      "2018-04-23 16:45:00 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-04-23 16:45:00 INFO  SparkContext:54 - Submitted application: Julia App on Spark\n",
      "2018-04-23 16:45:00 INFO  SecurityManager:54 - Changing view acls to: root\n",
      "2018-04-23 16:45:00 INFO  SecurityManager:54 - Changing modify acls to: root\n",
      "2018-04-23 16:45:00 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2018-04-23 16:45:00 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2018-04-23 16:45:00 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2018-04-23 16:45:00 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 36635.\n",
      "2018-04-23 16:45:00 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2018-04-23 16:45:00 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2018-04-23 16:45:00 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2018-04-23 16:45:00 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2018-04-23 16:45:00 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-6a9790d4-6979-4d74-af0a-e71413814487\n",
      "2018-04-23 16:45:00 INFO  MemoryStore:54 - MemoryStore started with capacity 408.9 MB\n",
      "2018-04-23 16:45:00 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2018-04-23 16:45:00 INFO  log:192 - Logging initialized @1932ms\n",
      "2018-04-23 16:45:00 INFO  Server:346 - jetty-9.3.z-SNAPSHOT\n",
      "2018-04-23 16:45:00 INFO  Server:414 - Started @1991ms\n",
      "2018-04-23 16:45:01 INFO  AbstractConnector:278 - Started ServerConnector@5e8f9e2d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-04-23 16:45:01 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2b62442c{/jobs,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@10c8f62{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@674c583e{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3f23a3a0{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5fb97279{/stages/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@439a8f59{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@25bcd0c7{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@32cb636e{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63cd604c{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@40dd3977{/storage,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3a4e343{/storage/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6a1d204a{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62dae245{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b6579e8{/environment,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6fff253c{/environment/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6c6357f9{/executors,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@591e58fa{/executors/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3954d008{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2f94c4db{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@593e824f{/static,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3a1d593e{/,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4a8a60bc{/api,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@314b8f2d{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@664a9613{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://4ed9b1a942b2:4040\n",
      "2018-04-23 16:45:01 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2018-04-23 16:45:01 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45065.\n",
      "2018-04-23 16:45:01 INFO  NettyBlockTransferService:54 - Server created on 4ed9b1a942b2:45065\n",
      "2018-04-23 16:45:01 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2018-04-23 16:45:01 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 4ed9b1a942b2, 45065, None)\n",
      "2018-04-23 16:45:01 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 4ed9b1a942b2:45065 with 408.9 MB RAM, BlockManagerId(driver, 4ed9b1a942b2, 45065, None)\n",
      "2018-04-23 16:45:01 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 4ed9b1a942b2, 45065, None)\n",
      "2018-04-23 16:45:01 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 4ed9b1a942b2, 45065, None)\n",
      "2018-04-23 16:45:01 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3003697{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:01 INFO  SparkContext:54 - Added JAR /opt/julia/v0.6/Spark/src/../jvm/sparkjl/target/sparkjl-0.1.jar at spark://4ed9b1a942b2:36635/jars/sparkjl-0.1.jar with timestamp 1524501901310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparkContext(local,Julia App on Spark)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Spark\n",
    "Spark.init()\n",
    "sc = SparkContext(master=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-23 16:45:03 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\n",
      "2018-04-23 16:45:03 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/root/volume/workspace/spark-warehouse').\n",
      "2018-04-23 16:45:03 INFO  SharedState:54 - Warehouse path is 'file:/root/volume/workspace/spark-warehouse'.\n",
      "2018-04-23 16:45:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@56febdc{/SQL,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3b8ee898{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1f86099a{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@77bb0ab5{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:03 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4248ed58{/static/sql,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:45:03 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n",
      "2018-04-23 16:45:05 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2018-04-23 16:45:05 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2018-04-23 16:45:05 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\n",
      "2018-04-23 16:45:05 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2018-04-23 16:45:06 INFO  CodeGenerator:54 - Code generated in 223.056877 ms\n",
      "2018-04-23 16:45:06 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 276.8 KB, free 408.6 MB)\n",
      "2018-04-23 16:45:06 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 408.6 MB)\n",
      "2018-04-23 16:45:06 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 4ed9b1a942b2:45065 (size: 23.2 KB, free: 408.9 MB)\n",
      "2018-04-23 16:45:06 INFO  SparkContext:54 - Created broadcast 0 from json at <unknown>:0\n",
      "2018-04-23 16:45:06 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194377 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2018-04-23 16:45:06 INFO  SparkContext:54 - Starting job: json at <unknown>:0\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Got job 0 (json at <unknown>:0) with 1 output partitions\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (json at <unknown>:0)\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents\n",
      "2018-04-23 16:45:06 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 9.4 KB, free 408.6 MB)\n",
      "2018-04-23 16:45:06 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.1 KB, free 408.6 MB)\n",
      "2018-04-23 16:45:06 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 4ed9b1a942b2:45065 (size: 5.1 KB, free: 408.9 MB)\n",
      "2018-04-23 16:45:06 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "2018-04-23 16:45:06 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks\n",
      "2018-04-23 16:45:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8328 bytes)\n",
      "2018-04-23 16:45:06 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)\n",
      "2018-04-23 16:45:06 INFO  Executor:54 - Fetching spark://4ed9b1a942b2:36635/jars/sparkjl-0.1.jar with timestamp 1524501901310\n",
      "2018-04-23 16:45:06 INFO  TransportClientFactory:267 - Successfully created connection to 4ed9b1a942b2/172.17.0.2:36635 after 33 ms (0 ms spent in bootstraps)\n",
      "2018-04-23 16:45:06 INFO  Utils:54 - Fetching spark://4ed9b1a942b2:36635/jars/sparkjl-0.1.jar to /tmp/spark-e6907d02-4e82-4dd1-a412-da25535f5819/userFiles-51edb8c1-caa7-4857-972b-26a9ad57de62/fetchFileTemp8000024820112854702.tmp\n",
      "2018-04-23 16:45:06 INFO  Executor:54 - Adding file:/tmp/spark-e6907d02-4e82-4dd1-a412-da25535f5819/userFiles-51edb8c1-caa7-4857-972b-26a9ad57de62/sparkjl-0.1.jar to class loader\n",
      "2018-04-23 16:45:06 INFO  FileScanRDD:54 - Reading File path: file:///usr/local/spark/examples/src/main/resources/people.json, range: 0-73, partition values: [empty row]\n",
      "2018-04-23 16:45:06 INFO  CodeGenerator:54 - Code generated in 12.653419 ms\n",
      "2018-04-23 16:45:06 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1953 bytes result sent to driver\n",
      "2018-04-23 16:45:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 245 ms on localhost (executor driver) (1/1)\n",
      "2018-04-23 16:45:06 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - ResultStage 0 (json at <unknown>:0) finished in 0.306 s\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Job 0 finished: json at <unknown>:0, took 0.345769 s\n",
      "2018-04-23 16:45:06 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2018-04-23 16:45:06 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2018-04-23 16:45:06 INFO  FileSourceStrategy:54 - Output Data Schema: struct<age: bigint, name: string>\n",
      "2018-04-23 16:45:06 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2018-04-23 16:45:06 INFO  CodeGenerator:54 - Code generated in 15.630174 ms\n",
      "2018-04-23 16:45:06 INFO  CodeGenerator:54 - Code generated in 23.616481 ms\n",
      "2018-04-23 16:45:06 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 276.8 KB, free 408.3 MB)\n",
      "2018-04-23 16:45:06 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.2 KB, free 408.3 MB)\n",
      "2018-04-23 16:45:06 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 4ed9b1a942b2:45065 (size: 23.2 KB, free: 408.8 MB)\n",
      "2018-04-23 16:45:06 INFO  SparkContext:54 - Created broadcast 2 from show at <unknown>:0\n",
      "2018-04-23 16:45:06 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194377 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2018-04-23 16:45:06 INFO  SparkContext:54 - Starting job: show at <unknown>:0\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Got job 1 (show at <unknown>:0) with 1 output partitions\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (show at <unknown>:0)\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[7] at show at <unknown>:0), which has no missing parents\n",
      "2018-04-23 16:45:06 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 11.1 KB, free 408.3 MB)\n",
      "2018-04-23 16:45:06 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.6 KB, free 408.3 MB)\n",
      "2018-04-23 16:45:06 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 4ed9b1a942b2:45065 (size: 5.6 KB, free: 408.8 MB)\n",
      "2018-04-23 16:45:06 INFO  SparkContext:54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1039\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at show at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "2018-04-23 16:45:06 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 1 tasks\n",
      "2018-04-23 16:45:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8328 bytes)\n",
      "2018-04-23 16:45:06 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 1)\n",
      "2018-04-23 16:45:06 INFO  FileScanRDD:54 - Reading File path: file:///usr/local/spark/examples/src/main/resources/people.json, range: 0-73, partition values: [empty row]\n",
      "2018-04-23 16:45:06 INFO  CodeGenerator:54 - Code generated in 10.477988 ms\n",
      "2018-04-23 16:45:06 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 1). 1262 bytes result sent to driver\n",
      "2018-04-23 16:45:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 38 ms on localhost (executor driver) (1/1)\n",
      "2018-04-23 16:45:06 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - ResultStage 1 (show at <unknown>:0) finished in 0.048 s\n",
      "2018-04-23 16:45:06 INFO  DAGScheduler:54 - Job 1 finished: show at <unknown>:0, took 0.052685 s\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession()\n",
    "df = read_json(spark, \"/usr/local/spark/examples/src/main/resources/people.json\")\n",
    "show(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-23 16:45:09 INFO  AbstractConnector:318 - Stopped Spark@5e8f9e2d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-04-23 16:45:09 INFO  SparkUI:54 - Stopped Spark web UI at http://4ed9b1a942b2:4040\n",
      "2018-04-23 16:45:09 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n",
      "2018-04-23 16:45:09 INFO  MemoryStore:54 - MemoryStore cleared\n",
      "2018-04-23 16:45:09 INFO  BlockManager:54 - BlockManager stopped\n",
      "2018-04-23 16:45:09 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n",
      "2018-04-23 16:45:09 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n",
      "2018-04-23 16:45:09 INFO  SparkContext:54 - Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "close(sc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
