{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia\n",
    "## Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 0.6.2\n",
      "Commit d386e40c17 (2017-12-13 18:08 UTC)\n",
      "Platform Info:\n",
      "  OS: Linux (x86_64-pc-linux-gnu)\n",
      "  CPU: Intel(R) Core(TM) i7-5820K CPU @ 3.30GHz\n",
      "  WORD_SIZE: 64\n",
      "  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell)\n",
      "  LAPACK: libopenblas64_\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-3.9.1 (ORCJIT, haswell)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPrecompiling module DataStructures.\n",
      "\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /usr/local/jdk1.8.0_172/jre/lib/amd64/server/libjvm.so\n",
      "2018-04-23 16:37:32 INFO  SparkContext:54 - Running Spark version 2.3.0\n",
      "2018-04-23 16:37:33 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-04-23 16:37:33 INFO  SparkContext:54 - Submitted application: Julia App on Spark\n",
      "2018-04-23 16:37:33 INFO  SecurityManager:54 - Changing view acls to: root\n",
      "2018-04-23 16:37:33 INFO  SecurityManager:54 - Changing modify acls to: root\n",
      "2018-04-23 16:37:33 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2018-04-23 16:37:33 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2018-04-23 16:37:33 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2018-04-23 16:37:33 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 41921.\n",
      "2018-04-23 16:37:33 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2018-04-23 16:37:33 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2018-04-23 16:37:33 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2018-04-23 16:37:33 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2018-04-23 16:37:33 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-a3f1d75e-1a46-463a-a7b0-4e8e1bc4a2b3\n",
      "2018-04-23 16:37:33 INFO  MemoryStore:54 - MemoryStore started with capacity 408.9 MB\n",
      "2018-04-23 16:37:33 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2018-04-23 16:37:33 INFO  log:192 - Logging initialized @1889ms\n",
      "2018-04-23 16:37:33 INFO  Server:346 - jetty-9.3.z-SNAPSHOT\n",
      "2018-04-23 16:37:33 INFO  Server:414 - Started @1947ms\n",
      "2018-04-23 16:37:33 INFO  AbstractConnector:278 - Started ServerConnector@fd46303{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-04-23 16:37:33 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@841e575{/jobs,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@25f7391e{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3f23a3a0{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5fb97279{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@439a8f59{/stages,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@61861a29{/stages/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@31024624{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63cd604c{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@40dd3977{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3a4e343{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6a1d204a{/storage,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62dae245{/storage/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b6579e8{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6fff253c{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6c6357f9{/environment,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@591e58fa{/environment/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3954d008{/executors,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2f94c4db{/executors/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@593e824f{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@72ccd81a{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6d8792db{/static,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@361c294e{/,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7859e786{/api,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5118388b{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@15a902e7{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:33 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://5bf6aa367e03:4040\n",
      "2018-04-23 16:37:33 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2018-04-23 16:37:33 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42821.\n",
      "2018-04-23 16:37:33 INFO  NettyBlockTransferService:54 - Server created on 5bf6aa367e03:42821\n",
      "2018-04-23 16:37:33 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2018-04-23 16:37:33 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 5bf6aa367e03, 42821, None)\n",
      "2018-04-23 16:37:33 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 5bf6aa367e03:42821 with 408.9 MB RAM, BlockManagerId(driver, 5bf6aa367e03, 42821, None)\n",
      "2018-04-23 16:37:33 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 5bf6aa367e03, 42821, None)\n",
      "2018-04-23 16:37:33 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 5bf6aa367e03, 42821, None)\n",
      "2018-04-23 16:37:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2d691f3d{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:34 INFO  SparkContext:54 - Added JAR /opt/julia/v0.6/Spark/src/../jvm/sparkjl/target/sparkjl-0.1.jar at spark://5bf6aa367e03:41921/jars/sparkjl-0.1.jar with timestamp 1524501454077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparkContext(local,Julia App on Spark)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Spark\n",
    "Spark.init()\n",
    "sc = SparkContext(master=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-23 16:37:41 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\n",
      "2018-04-23 16:37:41 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/root/volume/workspace/spark-warehouse/').\n",
      "2018-04-23 16:37:41 INFO  SharedState:54 - Warehouse path is 'file:/root/volume/workspace/spark-warehouse/'.\n",
      "2018-04-23 16:37:41 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7d151a{/SQL,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:41 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@294bdeb4{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:41 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@f2c488{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:41 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@54acff7d{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:41 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4564e94b{/static/sql,null,AVAILABLE,@Spark}\n",
      "2018-04-23 16:37:41 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n",
      "2018-04-23 16:37:43 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2018-04-23 16:37:43 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2018-04-23 16:37:43 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\n",
      "2018-04-23 16:37:43 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2018-04-23 16:37:43 INFO  CodeGenerator:54 - Code generated in 215.482726 ms\n",
      "2018-04-23 16:37:43 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 276.8 KB, free 408.6 MB)\n",
      "2018-04-23 16:37:43 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 408.6 MB)\n",
      "2018-04-23 16:37:43 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 5bf6aa367e03:42821 (size: 23.2 KB, free: 408.9 MB)\n",
      "2018-04-23 16:37:43 INFO  SparkContext:54 - Created broadcast 0 from json at <unknown>:0\n",
      "2018-04-23 16:37:43 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194377 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2018-04-23 16:37:43 INFO  SparkContext:54 - Starting job: json at <unknown>:0\n",
      "2018-04-23 16:37:43 INFO  DAGScheduler:54 - Got job 0 (json at <unknown>:0) with 1 output partitions\n",
      "2018-04-23 16:37:43 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (json at <unknown>:0)\n",
      "2018-04-23 16:37:43 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2018-04-23 16:37:43 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2018-04-23 16:37:43 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents\n",
      "2018-04-23 16:37:43 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 9.4 KB, free 408.6 MB)\n",
      "2018-04-23 16:37:43 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.1 KB, free 408.6 MB)\n",
      "2018-04-23 16:37:43 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 5bf6aa367e03:42821 (size: 5.1 KB, free: 408.9 MB)\n",
      "2018-04-23 16:37:43 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039\n",
      "2018-04-23 16:37:43 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "2018-04-23 16:37:43 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks\n",
      "2018-04-23 16:37:44 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8328 bytes)\n",
      "2018-04-23 16:37:44 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)\n",
      "2018-04-23 16:37:44 INFO  Executor:54 - Fetching spark://5bf6aa367e03:41921/jars/sparkjl-0.1.jar with timestamp 1524501454077\n",
      "2018-04-23 16:37:44 INFO  TransportClientFactory:267 - Successfully created connection to 5bf6aa367e03/172.17.0.2:41921 after 32 ms (0 ms spent in bootstraps)\n",
      "2018-04-23 16:37:44 INFO  Utils:54 - Fetching spark://5bf6aa367e03:41921/jars/sparkjl-0.1.jar to /tmp/spark-c1e6d447-0f44-4f25-9c86-867f077513b2/userFiles-23f0dd00-9cd2-4ebe-8dc1-f6ea8fb7b640/fetchFileTemp4871105350715324998.tmp\n",
      "2018-04-23 16:37:44 INFO  Executor:54 - Adding file:/tmp/spark-c1e6d447-0f44-4f25-9c86-867f077513b2/userFiles-23f0dd00-9cd2-4ebe-8dc1-f6ea8fb7b640/sparkjl-0.1.jar to class loader\n",
      "2018-04-23 16:37:44 INFO  FileScanRDD:54 - Reading File path: file:///usr/local/spark/examples/src/main/resources/people.json, range: 0-73, partition values: [empty row]\n",
      "2018-04-23 16:37:44 INFO  CodeGenerator:54 - Code generated in 18.219689 ms\n",
      "2018-04-23 16:37:44 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1953 bytes result sent to driver\n",
      "2018-04-23 16:37:44 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 267 ms on localhost (executor driver) (1/1)\n",
      "2018-04-23 16:37:44 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2018-04-23 16:37:44 INFO  DAGScheduler:54 - ResultStage 0 (json at <unknown>:0) finished in 0.320 s\n",
      "2018-04-23 16:37:44 INFO  DAGScheduler:54 - Job 0 finished: json at <unknown>:0, took 0.363632 s\n",
      "2018-04-23 16:37:44 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2018-04-23 16:37:44 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2018-04-23 16:37:44 INFO  FileSourceStrategy:54 - Output Data Schema: struct<age: bigint, name: string>\n",
      "2018-04-23 16:37:44 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2018-04-23 16:37:44 INFO  CodeGenerator:54 - Code generated in 15.005169 ms\n",
      "2018-04-23 16:37:44 INFO  CodeGenerator:54 - Code generated in 25.282633 ms\n",
      "2018-04-23 16:37:44 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 276.8 KB, free 408.3 MB)\n",
      "2018-04-23 16:37:44 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.2 KB, free 408.3 MB)\n",
      "2018-04-23 16:37:44 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 5bf6aa367e03:42821 (size: 23.2 KB, free: 408.8 MB)\n",
      "2018-04-23 16:37:44 INFO  SparkContext:54 - Created broadcast 2 from show at <unknown>:0\n",
      "2018-04-23 16:37:44 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194377 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2018-04-23 16:37:44 INFO  SparkContext:54 - Starting job: show at <unknown>:0\n",
      "2018-04-23 16:37:44 INFO  DAGScheduler:54 - Got job 1 (show at <unknown>:0) with 1 output partitions\n",
      "2018-04-23 16:37:44 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (show at <unknown>:0)\n",
      "2018-04-23 16:37:44 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2018-04-23 16:37:44 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2018-04-23 16:37:44 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[7] at show at <unknown>:0), which has no missing parents\n",
      "2018-04-23 16:37:44 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 11.1 KB, free 408.3 MB)\n",
      "2018-04-23 16:37:44 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.6 KB, free 408.3 MB)\n",
      "2018-04-23 16:37:44 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 5bf6aa367e03:42821 (size: 5.6 KB, free: 408.8 MB)\n",
      "2018-04-23 16:37:44 INFO  SparkContext:54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1039\n",
      "2018-04-23 16:37:44 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at show at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "2018-04-23 16:37:44 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 1 tasks\n",
      "2018-04-23 16:37:44 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8328 bytes)\n",
      "2018-04-23 16:37:44 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 1)\n",
      "2018-04-23 16:37:44 INFO  FileScanRDD:54 - Reading File path: file:///usr/local/spark/examples/src/main/resources/people.json, range: 0-73, partition values: [empty row]\n",
      "2018-04-23 16:37:44 INFO  CodeGenerator:54 - Code generated in 10.764948 ms\n",
      "2018-04-23 16:37:44 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 1). 1262 bytes result sent to driver\n",
      "2018-04-23 16:37:44 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 39 ms on localhost (executor driver) (1/1)\n",
      "2018-04-23 16:37:44 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2018-04-23 16:37:44 INFO  DAGScheduler:54 - ResultStage 1 (show at <unknown>:0) finished in 0.050 s\n",
      "2018-04-23 16:37:44 INFO  DAGScheduler:54 - Job 1 finished: show at <unknown>:0, took 0.053231 s\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession()\n",
    "df = read_json(spark, \"/usr/local/spark/examples/src/main/resources/people.json\")\n",
    "show(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-23 16:37:51 INFO  AbstractConnector:318 - Stopped Spark@fd46303{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-04-23 16:37:51 INFO  SparkUI:54 - Stopped Spark web UI at http://5bf6aa367e03:4040\n",
      "2018-04-23 16:37:51 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n",
      "2018-04-23 16:37:51 INFO  MemoryStore:54 - MemoryStore cleared\n",
      "2018-04-23 16:37:51 INFO  BlockManager:54 - BlockManager stopped\n",
      "2018-04-23 16:37:51 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n",
      "2018-04-23 16:37:51 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n",
      "2018-04-23 16:37:51 INFO  SparkContext:54 - Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "close(sc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
